# -*- coding: utf-8 -*-
"""Final Complete System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0Bzs53WO22k-waVpfdHlCWUhNelllTXBBSmZleEZDUV9MNFVF
"""

# %matplotlib inline
from matplotlib import pyplot as plt
import keras
from keras.layers import Input, Dense, Reshape, Flatten, GRU
from keras.activations import relu, softmax, linear
from keras import Sequential, Model
from keras.optimizers import Adam, SGD, RMSprop, Nadam
import tensorflow as tf
from IPython.display import clear_output
import sys
import numpy as np
import random
from time import time

#def rewards(degree,steps):
  #  p=-degree**(steps+1)
   # return p




    
    


env = Environment()
print("Fire Location: ", env.F)




from rl.agents.dqn import DQNAgent
from rl.agents.ddpg import DDPGAgent
from rl.agents.sarsa import SARSAAgent
from rl.agents.cem import CEMAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory, Memory
from tensorforce.agents import PPOAgent
from keras.models import load_model

EPISODES = 500

agent_classes = [DQNAgent]


agent_specs = [

]    
    
for agent_class, agent_spec in zip(agent_classes, agent_specs):
    agent_type = agent_spec["type"]
    del agent_spec["type"]
    
    name = agent_spec["name"]
    del agent_spec["name"]

    # Env
    env = Environment(max_steps=1000)

    # Model Definition
    if agent_type == "keras-rl":
        the_input = Input((1, ) + env.render().shape)
        flatten = Flatten()(the_input)
        x = Dense(128, activation='relu')(flatten)
        x = Dense(256, activation='relu')(x)
        x = Dense(256, activation='relu')(x)
        x = Dense(256, activation='relu')(x)
        x = Dense(25, activation='linear')(x)
        
        model = Model(inputs=[the_input], outputs=[x])
        #model.load_weights('pretrained.h5')
        model.compile(optimizer='adam', loss='mse')
        agent_spec["model"] = model
  

        # Agent Init
        agent = agent_class(**agent_spec)
        print("Starting experiment for %s." % name)

        # Agent Train
        agent.compile(Adam(lr=1e-2), metrics=['mse'])
        history = agent.fit(env, nb_steps=EPISODES*250, nb_max_episode_steps=1000, visualize=False, verbose=2)


        # Fetch Train Summary
        summary_step = history.history["nb_episode_steps"][:EPISODES]

        # Plotting
        perf_plot.new(name)
        start_time=time()
        for episode, steps in enumerate(summary_step):
            perf_plot.log(episode, steps)
        print("Model: %s, Average Steps: %s, Minimum Steps: %s, Time: %.3f secs" % (name, np.average(summary_step), np.amin(summary_step), (time() - start_time)))
            
    elif agent_type == "tensorforce":
        # Tensorforce agent
        
        # Init State Space Definition
        the_input = env.render().shape
        agent_spec["states"] = dict(type='float', shape=the_input)
        
        # Agent Init
        agent = agent_class(**agent_spec)
        print("Starting experiment for %s." % name)
        
        
        # Get prediction from agent, execute
        perf_plot.new(name)
        for episode in range(EPISODES):
            t = False
            env.reset()
            steps = np.zeros(EPISODES)
            while t is False:
                action = agent.act(env.render())
                _from = int(action % 5) #   // % is the "modulo operator", the remainder of i / width;
                _to = int(action / 5) #   // where "/" is an integer division
                steps[episode] += 1  

                if steps[episode] > 1000:
                    t = True
                    break

                s1, r, t, _ = env.step(action)
                agent.observe(reward=r, terminal=t)

            perf_plot.log(episode, steps[episode])
            print("Steps=%s, Episode=%s" % (steps[episode], episode))
        
    else:
        raise Exception("No agent type defined. should be keras-rl or tensorforce (Or somthing custom made which should be handled)")

from rl.agents.dqn import DQNAgent
from rl.agents.ddpg import DDPGAgent
from rl.agents.sarsa import SARSAAgent
from rl.agents.cem import CEMAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory, Memory
from tensorforce.agents import PPOAgent
from keras.models import load_model

EPISODES = 500

agent_classes = [DQNAgent]


agent_specs = [
    
    #DQN
#     dict(
#         model=None, # Will be set later
#         nb_actions=25,
#         memory=SequentialMemory(limit=50000, window_length=1),
#         nb_steps_warmup=60,
#         target_model_update=1e-4,
#         enable_double_dqn=False,
#         policy=None,
#         batch_size=32,
#         name="QMP-DQN (Proposed)",
#         type="keras-rl"
#     ),
    
    #Dueling DQN 
    dict(
        model=None, # Will be set later
        nb_actions=196,
        memory=SequentialMemory(limit=50000, window_length=1),
        nb_steps_warmup=100,
        target_model_update=1e-4,
        enable_dueling_network=True,
        dueling_type='avg',
        policy=None,
        batch_size=128,
        name="QMP-DQN (Proposed)",
        type="keras-rl"
    ),
      
    #DDQN 
#     dict(
#         model=None, 
#         nb_actions=25, 
#         memory=SequentialMemory(limit=50000, window_length=1), 
#         nb_steps_warmup=60, 
#         target_model_update=1e-4, 
#         policy=None, 
#         name="DDQN",
#         type="keras-rl"
#     ),
]    
    
for agent_class, agent_spec in zip(agent_classes, agent_specs):
    agent_type = agent_spec["type"]
    del agent_spec["type"]
    
    name = agent_spec["name"]
    del agent_spec["name"]

    # Env
    env = Environment(max_steps=1000)

    # Model Definition
    if agent_type == "keras-rl":
        the_input = Input((1, ) + env.render().shape)
        flatten = Flatten()(the_input)
        x = Dense(256, activation='relu')(flatten)
        x = Dense(1024, activation='relu')(x)
        x = Dense(1024, activation='relu')(x)
        x = Dense(1024, activation='relu')(x)
        x = Dense(196, activation='linear')(x)
        
        model = Model(inputs=[the_input], outputs=[x])
        model.load_weights('pretrained.h5')
        model.compile(optimizer='adam', loss='mse')
        agent_spec["model"] = model
  

        # Agent Init
        agent = agent_class(**agent_spec)
        print("Starting experiment for %s." % name)

        # Agent Train
        agent.compile(Adam(lr=1e-2), metrics=['mse'])
        history = agent.fit(env, nb_steps=EPISODES*150, nb_max_episode_steps=1000, visualize=False, verbose=2)


        # Fetch Train Summary
        summary_step = history.history["nb_episode_steps"][:EPISODES]

        # Plotting
        perf_plot.new(name)
        start_time=time()
        for episode, steps in enumerate(summary_step):
            perf_plot.log(episode, steps)
        print("Model: %s, Average Steps: %s, Minimum Steps: %s, Time: %.3f secs" % (name, np.average(summary_step), np.amin(summary_step), (time() - start_time)))
            
    elif agent_type == "tensorforce":
        # Tensorforce agent
        
        # Init State Space Definition
        the_input = env.render().shape
        agent_spec["states"] = dict(type='float', shape=the_input)
        
        # Agent Init
        agent = agent_class(**agent_spec)
        print("Starting experiment for %s." % name)
        
        
        # Get prediction from agent, execute
        perf_plot.new(name)
        for episode in range(EPISODES):
            t = False
            env.reset()
            steps = np.zeros(EPISODES)
            while t is False:
                action = agent.act(env.render())
                _from = int(action % 5) #   // % is the "modulo operator", the remainder of i / width;
                _to = int(action / 5) #   // where "/" is an integer division
                steps[episode] += 1  

                if steps[episode] > 1000:
                    t = True
                    break

                s1, r, t, _ = env.step(action)
                agent.observe(reward=r, terminal=t)

            perf_plot.log(episode, steps[episode])
            print("Steps=%s, Episode=%s" % (steps[episode], episode))
        
    else:
        raise Exception("No agent type defined. should be keras-rl or tensorforce (Or somthing custom made which should be handled)")

from rl.agents.sarsa import SARSAAgent
from rl.memory import SequentialMemory, Memory
from tensorforce.agents import PPOAgent
from tensorforce.agents import VPGAgent
from keras.models import load_model

EPISODES = 500
env = Environment()
input_shape = env.render().shape

agent_classes = [PPOAgent, VPGAgent]


agent_specs = [
    # PPO
    dict(
        states=dict(type='float', shape=input_shape),
        actions=dict(type='int', num_actions=25),
        network=[dict(type='dense', size=128, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
                ],
        step_optimizer=dict(
            type='adam',
            learning_rate=1e-6
        ),
        batching_capacity=32,
        subsampling_fraction=0.25,
        likelihood_ratio_clipping=0.1,
        optimization_steps=25,
        update_mode=dict(
            unit='timesteps',
            batch_size=1,
            frequency=1
        ),
        name="PPO",
        type="tensorforce"
    ),
    # VPG
    dict(
        states=dict(type='float', shape=input_shape),
        actions=dict(type='int', num_actions=25),
        network=[dict(type='dense', size=128, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
            dict(type='dense', size=256, activation='relu'),
                ],
        optimizer=dict(
            type='adam',
            learning_rate=1e-6
        ),
        batching_capacity=32,
        update_mode=dict(
            unit='timesteps',
            batch_size=1,
            frequency=1
        ),
        name="VPG",
        type="tensorforce"
    )
    
]    
    

# Random Agent
no_logging = False
if not no_logging:
    perf_plot.new("Random Agent")
for i in range(500):
    steps = 0
    s = env.reset()
    terminal = False
    while not terminal:
        s = env.render() # 1

        actions = np.random.randint(25)

        old_state = s
        
        if steps > 1000:
            terminal = True
            break

        s1, r, t, _ = env.step(actions)
        s = s1
        steps += 1
        if t is True:
            terminal = t
            break

    if not no_logging:
        perf_plot.log(i, steps)

import sys
import gym
import pylab
import numpy as np
from keras.layers import Dense
from keras.models import Sequential
from keras.optimizers import Adam

EPISODES = 500
no_logging = False

# A2C(Advantage Actor-Critic) agent for the Cartpole
class A2CAgent:
    def __init__(self, state_size, action_size):
        # if you want to see Cartpole learning, then change to True
        self.render = True
        self.load_model = False
        # get size of state and action
        self.state_size = state_size
        self.action_size = action_size
        self.value_size = 1

        # These are hyper parameters for the Policy Gradient
        self.discount_factor = 0.99
        self.actor_lr = 0.001
        self.critic_lr = 0.005

        # create model for policy network
        self.actor = self.build_actor()
        self.critic = self.build_critic()

        if self.load_model:
            self.actor.load_weights("./save_model/cartpole_actor.h5")
            self.critic.load_weights("./save_model/cartpole_critic.h5")

    # approximate policy and value using Neural Network
    # actor: state is input and probability of each action is output of model
    def build_actor(self):
        actor = Sequential()
        actor.add(Dense(24, input_dim=self.state_size, activation='relu',
                        kernel_initializer='he_uniform'))
        actor.add(Dense(self.action_size, activation='softmax',
                        kernel_initializer='he_uniform'))
        actor.summary()
        # See note regarding crossentropy in cartpole_reinforce.py
        actor.compile(loss='categorical_crossentropy',
                      optimizer=Adam(lr=self.actor_lr))
        return actor

    # critic: state is input and value of state is output of model
    def build_critic(self):
        critic = Sequential()
        critic.add(Dense(24, input_dim=self.state_size, activation='relu',
                         kernel_initializer='he_uniform'))
        critic.add(Dense(self.value_size, activation='linear',
                         kernel_initializer='he_uniform'))
        critic.summary()
        critic.compile(loss="mse", optimizer=Adam(lr=self.critic_lr))
        return critic

    # using the output of policy network, pick action stochastically
    def get_action(self, state):
        policy = self.actor.predict(state, batch_size=1).flatten()
        return np.random.choice(self.action_size, 1, p=policy)[0]

    # update policy network every episode
    def train_model(self, state, action, reward, next_state, done):
        target = np.zeros((1, self.value_size))
        advantages = np.zeros((1, self.action_size))

        value = self.critic.predict(state)[0]
        next_value = self.critic.predict(next_state)[0]

        if done:
            advantages[0][action] = reward - value
            target[0][0] = reward
        else:
            advantages[0][action] = reward + self.discount_factor * (next_value) - value
            target[0][0] = reward + self.discount_factor * next_value

        self.actor.fit(state, advantages, epochs=1, verbose=0)
        self.critic.fit(state, target, epochs=1, verbose=0)


if __name__ == "__main__":
    # In case of CartPole-v1, maximum length of episode is 500
    env = Environment()
    # get size of state and action from environment
    state_size = env.render().shape[0]
    action_size = 25

    # make A2C agent
    agent = A2CAgent(state_size, action_size)

    scores, episodes = [], []
    if not no_logging:
        perf_plot.new("A2C")
    for e in range(EPISODES):
        done = False
        score = 0
        steps = 0
        state = env.reset()
        state = np.reshape(state, [1, state_size])

        while not done:
            if agent.render:
                env.render()

            action = agent.get_action(state)
            next_state, reward, done, info = env.step(action)
            next_state = np.reshape(next_state, [1, state_size])
           

            agent.train_model(state, action, reward, next_state, done)

            score += reward
            state = next_state
            steps += 1
            if steps > 1000:
                done = True
                break
                # every episode, plot the play time
                #score = score if score == 500.0 else score + 100
                #pylab.plot(episodes, scores, 'b')
                #pylab.savefig("./save_graph/cartpole_a2c.png")
        print("episode:", e, "  steps:", steps)

        if not no_logging:
            perf_plot.log(e, steps)

